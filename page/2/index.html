<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="SJTU | PhD Candidate">
<meta name="keywords" content=".">
<meta property="og:type" content="website">
<meta property="og:title" content="保持动力">
<meta property="og:url" content="jrunning.cn/page/2/index.html">
<meta property="og:site_name" content="保持动力">
<meta property="og:description" content="SJTU | PhD Candidate">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="保持动力">
<meta name="twitter:description" content="SJTU | PhD Candidate">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="jrunning.cn/page/2/"/>





  <title>保持动力</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">保持动力</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">寻找稳固的自我</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-study">
          <a href="/study/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-book"></i> <br />
            
            论文
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-nav">
          <a href="/nav/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-map"></i> <br />
            
            导航
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="jrunning.cn/2018/12/15/paper-comparing-sift-and/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiang Shuo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="保持动力">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/15/paper-comparing-sift-and/" itemprop="url">论文笔记 | 专利图像表示：SIFT vs SURF</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-15T11:09:00+08:00">
                2018-12-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper/" itemprop="url" rel="index">
                    <span itemprop="name">paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="比较SIFT与SURF在专利图像上的性能"><a href="#比较SIFT与SURF在专利图像上的性能" class="headerlink" title="比较SIFT与SURF在专利图像上的性能"></a><strong>比较SIFT与SURF在专利图像上的性能</strong></h2><blockquote>
<p>Lindqvist C. <strong>Comparing SIFT and SURF: Performance on patent drawings</strong>[D]. 2017.</p>
</blockquote>
<p>这是来自瑞典乌普萨拉大学学生的一篇硕士论文，关于专利图像表示，比较SIFT与SURF。从SURF的介绍中，我们知道SURF与SIFT算法相似，SIFT算法比较稳定，检测特征点更多，但是复杂度较高，而SURF要运算简单，效率高，运算时间短一点。本文的作者想通过实验，在文档图像上进行验证。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/ch4w0ufumohgr9a8ro09m27n/image_1cunslriu1it71bq114j2a61s9k15.png" alt="image_1cunslriu1it71bq114j2a61s9k15.png-56.6kB"></p>
<h3 id="★-摘要"><a href="#★-摘要" class="headerlink" title="★ 摘要"></a><strong>★ 摘要</strong></h3><p>近年来，有研究发现人们可以利用专利中的图像来组织大规模的专利，这对于减少浏览专利所花费的时间和工作量是极有帮助的。研究提出了一个系统，可以通过CBIR技术寻找和比较特定的图像。关于CBIR技术，有非常多可以使用的算法，而它们各有各的优势。这篇论文测试了两种关于专利图像表示的算法：<a href="https://blog.csdn.net/songzitea/article/details/13627823" target="_blank" rel="noopener">SIFT</a>和<a href="https://blog.csdn.net/songzitea/article/details/16986423" target="_blank" rel="noopener">SURF</a>。</p>
<p>实验显示，当我们仅看前20个算法相关结果时，可以检索到3-4个实际相关的图像，当目标范围更大时，检索到的相关图像可能会更多。这意味着可能可以仅使用一个特定专利文档的图像来找到多个相关专利。</p>
<h3 id="★-结论"><a href="#★-结论" class="headerlink" title="★ 结论"></a><strong>★ 结论</strong></h3><p>本文的工作首先是收集数据，其次是选择合适的系统，不仅要保证速度和准确率，而且最好有开源代码进行调整。最终，选择了SURF和SIFT。使用它们进行检索时，前20个相关的结果中有3-4个是实际相关的，查看这3-4个实际相关的图像归属，可以找到1-2个相关的专利文档。但是，是否可以推广到大规模的知识产权领域，有待进一步的验证。</p>
<p>重复上述的过程，可以找到相似图像。然而花费的时间与找到的数量，相比于使用关键词检索再一一浏览，有何差异，这需要进一步的评估。</p>
<p>SIFT和SURF都可以有效地处理这一问题，总体相差不大，见下表所示。如果考虑时间，SURF要优于SIFT。相反，如果不考虑时间，SIFT的准确率会更高一些。所以具体如何选择，需要根据不同的环境来考虑。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/oz7d28n2brunpoyaof4dk5ap/image_1cunsn25jvhqg1412hi7p0131j1l.png" alt="image_1cunsn25jvhqg1412hi7p0131j1l.png-168.3kB"></p>
<h3 id="★-其他"><a href="#★-其他" class="headerlink" title="★ 其他"></a><strong>★ 其他</strong></h3><ul>
<li>CBIR常用的图像数据库：<a href="https://sites.google.com/site/dctresearch/Home/content-based-image-retrieval" target="_blank" rel="noopener"><strong>The COREL Database</strong></a>，包含了10800张图像，80个类别。</li>
<li>本文中使用了<a href="http://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_manual-1.8.4.pdf" target="_blank" rel="noopener"><strong>FLANN库</strong></a>来匹配特征。FLANN库全称是Fast Library for Approximate Nearest Neighbors，它是目前最完整的（近似）最近邻开源库。不但实现了一系列查找算法，还包含了一种自动选取最快算法的机制。</li>
<li>使用不同的数据集（系统）进行评估，举例：PatMedia，PATSEEK；本文中用了TREC的评估标准。</li>
</ul>
<h3 id="★-自己的收获"><a href="#★-自己的收获" class="headerlink" title="★ 自己的收获"></a><strong>★ 自己的收获</strong></h3><ol>
<li>对SIFT和SURF的比较有了感性的认识，再文档图像的应用中，总体差别不大。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="jrunning.cn/2018/12/14/paper-what-is-the-right/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiang Shuo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="保持动力">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/14/paper-what-is-the-right/" itemprop="url">论文笔记 | 文档图像表示：三种方法比较</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-14T21:53:00+08:00">
                2018-12-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper/" itemprop="url" rel="index">
                    <span itemprop="name">paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="文档图像的正确表示方式"><a href="#文档图像的正确表示方式" class="headerlink" title="文档图像的正确表示方式"></a><strong>文档图像的正确表示方式</strong></h2><blockquote>
<p>Csurka G, Larlus D, Gordo A, et al. <strong>What is the right way to represent document images?</strong>[J]. arXiv preprint arXiv:1603.01076, 2016.</p>
</blockquote>
<p>在前几天读过的一篇专利图像分类的论文中（<a href="http://jacobrun.com/2018/12/04/2018-12-4-paper-document-image-classification/" target="_blank" rel="noopener">笔记链接</a>），讲到了两种基于浅层特征的图像表示方法：Run Length和Fisher Vector。本文从更全面的视角，比较了浅层特征图像表示方法、深层特征图像表示方法（基于卷积神经网络）以及基于前两类特征的混合表示方法。</p>
<h3 id="★-摘要"><a href="#★-摘要" class="headerlink" title="★ 摘要"></a><strong>★ 摘要</strong></h3><p>本文探讨了基于视觉特征的图像表示方法，通过广泛的实验研究对三种表示方法进行了比较：</p>
<ul>
<li>传统浅层特征：Run Length及Fisher Vector</li>
<li>深层特征：基于卷积神经网络的表示</li>
<li>从混合结构中提取的特征</li>
</ul>
<p>通过不同的任务（分类、聚类及检索）、不同的情境（领域迁移）、使用不同的数据集对这些特征表示方法进行了评估。<strong>结果表明：在没有领域迁移的情况下，即新任务与训练模型的任务较为接近时，深层特征表现较好；而在领域范围较大或有领域迁移的情况下，浅层特征Fisher Vector表现较好。</strong></p>
<h3 id="★-引言"><a href="#★-引言" class="headerlink" title="★ 引言"></a><strong>★ 引言</strong></h3><p>文档图像的理解与表示，通常有三个切入点线索：视觉线索、结构线索、文本线索。视觉线索展现的是一个图片的整体形貌，通过“整体一瞥”的方式来区分图片。结构线索捕获文档不同部分之间的关系，比如布局分析等。文本线索是从文档中获取文本信息，其中往往包括了语义信息。</p>
<p>同时使用三种线索会得到最好的效果，然而获取结构线索及语义线索的计算代价很大，在大规模领域里不适用。比如结构线索需要对文档布局进行分析，文本线索需要对整个文档执行OCR，它们都慢而易错。而且，这两种线索往往都限定在特定的领域，不具备迁移性。</p>
<p>视觉特征更一般化而且往往可以很快速地获得，因此在文档理解工作中被大量使用，有时再将另外两类信息结合进去。最近，深度学习技术被用于建立文档的视觉表示，在分类和检索任务中取得了比传统浅层特征、手工特征等更好的效果。但是深度学习对于文档图像的表示，还残留了两个问题至今没有解决：对于给定的任务与数据集，深度特征在所有的案例上都比浅层特征要好吗？它对于领域迁移、任务迁移以及数据集迁移，表现的都更好吗？</p>
<p>此外，也有研究提出了将深度表示模型与浅层特征结合，在浅层特征的顶层加上深度表示特征，旨在结合两者的优点。</p>
<h3 id="★-相关工作"><a href="#★-相关工作" class="headerlink" title="★ 相关工作"></a><strong>★ 相关工作</strong></h3><p>传统视觉特征是基于图像像素的简单统计规律。更详尽的浅层特征表示例如RunLength，结合金字塔方法，对于一些任务有更好的准确性。受自然图像表示的启发，bag-of-visual-words（BoV）或Fisher Vector表示方法，它们基于SIFT或SURF描述符，具有更好的泛化能力。</p>
<p>近期，基于CNN的表示方法，在分类和检索的性能上要好于BoV。深度特征的特点是端到端的学习，这意味着特征结构和预测被合并到同一个步骤中。也就是说，特征以及分类器被联合学习，不能被分开了。这种表示方法被大范围地证明要好于浅层特征，但是它们往往针对特定领域，对于通用的文档图像表示还没有被详细研究。而且训练的速度非常慢。</p>
<p>混合CNN以及FV的表示方法在近期被提出，这种方法被证明有较好的迁移性，但是在文档图像中还没有得到应用。</p>
<h3 id="★-文档图像特征表示"><a href="#★-文档图像特征表示" class="headerlink" title="★ 文档图像特征表示"></a><strong>★ 文档图像特征表示</strong></h3><h4 id="☆-Run-Length"><a href="#☆-Run-Length" class="headerlink" title="☆ Run Length"></a><strong>☆ Run Length</strong></h4><blockquote>
<p>Chan Y K, Chang C C. <strong>Image matching using run-length feature</strong>[J]. Pattern Recognition Letters, 2001, 22(5): 447-455.</p>
</blockquote>
<p>具体细节理解可以参见：<a href="http://jacobrun.com/2018/12/04/2018-12-4-paper-document-image-classification/" target="_blank" rel="noopener">笔记链接</a></p>
<h4 id="☆-Fisher-Vector"><a href="#☆-Fisher-Vector" class="headerlink" title="☆ Fisher Vector"></a><strong>☆ Fisher Vector</strong></h4><blockquote>
<p>Perronnin F, Dance C. <strong>Fisher kernels on visual vocabularies for image categorization</strong>[C]//2007 IEEE conference on computer vision and pattern recognition. IEEE, 2007: 1-8.</p>
</blockquote>
<p><img src="http://static.zybuluo.com/jiangshuo1016/sy5qgq2413g74qp75fz92j4w/QQ%E6%88%AA%E5%9B%BE20181213102949.png" alt="QQ截图20181213102949.png-57.6kB"></p>
<p>FV可以看作是BoV的扩展，BoV仅仅使用0阶统计规律，而FV编码了更高阶的信息。与BoV相似，FV依赖于中间的表示：视觉词汇表，它可以被视作描述图像中低阶描述符排布的概率密度函数。我们使用高斯混合模型（GMM）来表示密度。</p>
<p>对于一个核函数，有：</p>
<script type="math/tex; mode=display">K\left(X_{i},X_{j}\right)=\left[\Gamma_{\lambda}\left(I\right)\right]^{\top}\left[\Gamma_{\lambda}\left(J\right)\right]</script><p>这个$\Gamma_{\lambda}\left(I\right)$就是Fisher Kernel核函数的表示方法，即Fisher Vector，它是由Fisher Score归一化得到的。核函数需要满足：$K\left(X_{i},X_{i}\right)=1$，因此对Fisher Score需要进行归一化，形式如下：</p>
<script type="math/tex; mode=display">\Gamma_{\lambda}\left(I\right)=F_{\lambda}^{ -\frac{1}{2} }G_{\lambda}\left(I\right)</script><p>其中，$F_{\lambda}$是Fisher信息矩阵，$G_{\lambda}$是Fisher Score，定义：</p>
<script type="math/tex; mode=display">G_{\lambda}\left(I\right)=\frac{1}{T}\sum_{t=1}^{T}\nabla_{\lambda}\log\left\{ \sum_{n=1}^{N}\omega_{n}p\left(x_{t}\mid\mu_{n},{\scriptstyle \sum_{n}}\right)\right\}</script><p>这里，$X_{I}=\left\{ x_{t}\right\} _{t=1}^{T}$是从图形$I$中提取的低维特征，特征间独立同分布，服从于分布$p$，是一个GMM。$\mu_{n}$和$\sum_{n}$是均值和协方差，都是GMM的模型参数，$\omega_{n}$是GMM中第$n$个量的权重。这个$\log$似然函数对$\lambda$的梯度，描述了参数$\lambda$在$p$生成特征点集合$X$的过程中如何作用，所以这个Fisher Score中也包含了GMM生成$X$的过程中的一些结构化的信息。</p>
<p>Fisher信息矩阵，是用来作归一化的，其计算方法为：</p>
<script type="math/tex; mode=display">F_{\lambda}=G_{\lambda}G_{\lambda}^{\top}</script><p>分别对$G_{\lambda}$求关于均值与协方差的梯度，我们可以得到：</p>
<script type="math/tex; mode=display">\Gamma_{ \mu_{n}^{d} }\left(I\right)=\frac{1}{ T\sqrt{ \omega_{n} } }\sum_{t=1}^{T}g_{n}\left(x_{t}\right)\left(\frac{ x_{t}^{d}-\mu_{n}^{d} } { s_{n}^{d} }\right)</script><script type="math/tex; mode=display">\Gamma_{ s_{n}^{d} }\left(I\right)=\frac{1}{ T\sqrt{ 2\omega_{n} } }\sum_{t=1}^{T}g_{n}\left(x_{t}\right)\left[\frac{ \left(x_{t}^{d}-\mu_{n}^{d}\right)^{2} }{ \left(s_{n}^{d}\right)^{2} }-1\right]</script><p>其中，${\scriptstyle g_{n}\left(x_{t}\right)}=\frac{ {\scriptstyle \omega_{n} } {\scriptstyle p\left({\scriptstyle x\mid\mu_{n},{\scriptstyle {\scriptscriptstyle \sum}_{n} } }\right)} } { { \scriptstyle \sum_{j=1}^{N}\omega_{j}p\left(x\mid\mu_{j},{\scriptstyle {\scriptscriptstyle \sum_{j} } }\right)} }$，$s_n^d$是$\sum_n$的对角线元素。联合$\Gamma_{\mu_{n}^{d} }\left(I\right),\Gamma_{s_{n}^{d} }\left(I\right)$以及$\Gamma_{\lambda}\left(I\right)$，最终得到的特征一共是$2ND$维，$D$是低维特征$x_t$的维度。然后对最终的$2ND$维特征向量进行l2范数规范化，获得最终的Fisher Vector。</p>
<p><strong>可以看到，$D$维向量$x_t$中的每一个值，都与均值和方差做运算，并加上权重，Fisher Vector中包含了原特征向量每一维的值，并且包含了生成式建模过程的结构性信息，对图片的表达更加细致。</strong></p>
<h4 id="☆-卷积神经网络"><a href="#☆-卷积神经网络" class="headerlink" title="☆ 卷积神经网络"></a><strong>☆ 卷积神经网络</strong></h4><p>卷积神经网络，在端到端的模式下，通过多个线性层或非线性层的联合学习，实现解决某一特殊任务。<a href="https://blog.csdn.net/langb2014/article/details/53019350" target="_blank" rel="noopener">端到端</a>，指的是输入是原始数据，输出是最后的结果，原来输入端不是直接的原始数据，而是在原始数据中提取的特征。其好处是：通过缩减人工预处理和后续处理，尽可能使模型从原始输入到最终输出，给模型更多可以根据数据自动调节的空间，增加模型的整体契合度。</p>
<p>卷积神经网络，通常包括卷积层、全连接层以及作为最终层的Softmax层。一个前馈的神经网络可以被视作是一系列嵌套的函数：</p>
<script type="math/tex; mode=display">{\displaystyle F\left(x\right)=F_{L}\left(\ldots F_{2}\left(F_{1}\left(x,W_{1}\right),W_{2}\right),\ldots,W_{L}\right)}</script><p><img src="http://static.zybuluo.com/jiangshuo1016/2k8in3w5o9xyb9txm0bk71u6/image_1cuj5qe0f1ti81l9livnol1c68l.png" alt="image_1cuj5qe0f1ti81l9livnol1c68l.png-158.9kB"></p>
<blockquote>
<p>关于卷积神经网络细节介绍，可以参考这份PPT：<a href="https://wenku.baidu.com/view/b0b2bf5800f69e3143323968011ca300a6c3f68f.html" target="_blank" rel="noopener">Introduction to CNNs and AlexNet</a></p>
</blockquote>
<p>尽管卷积神经网络在1990年左右就被提出（LeNet），但是随着最近它被广泛成功应用于图像识别竞赛，越来越多不同结构的卷积神经网络被提出。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/sc4tiy8swqbuvv19efulyf53/QQ%E6%88%AA%E5%9B%BE20181213150616.png" alt="QQ截图20181213150616.png-226.7kB"></p>
<p>本文聚焦于两种流行的CNN框架：$AlexNet$（左上）和$GoogLeNet$（左下）。</p>
<p>※$AlexNet$：<a href="https://blog.csdn.net/qq_24695385/article/details/80368618" target="_blank" rel="noopener">详细解读</a><br>※$GoogLeNet$：<a href="https://blog.csdn.net/cdknight_happy/article/details/79247280" target="_blank" rel="noopener">详细解读</a></p>
<h4 id="☆-混合描述符"><a href="#☆-混合描述符" class="headerlink" title="☆ 混合描述符"></a><strong>☆ 混合描述符</strong></h4><blockquote>
<p>Perronnin F, Larlus D. <strong>Fisher vectors meet neural networks: A hybrid classification architecture</strong>[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3743-3752.</p>
</blockquote>
<p><img src="http://static.zybuluo.com/jiangshuo1016/h5fws7ov8h7zjqb5s7kbcbs9/QQ%E6%88%AA%E5%9B%BE20181213161231.png" alt="QQ截图20181213161231.png-64.8kB"></p>
<p>混合描述符表示结合了FV的无监督部分以及CNN的有监督部分，前半部分照搬上文讲解的FV特征，后面通过L-1个全连接层以及最终的softmax层输出。同样的，前面的无监督部分也可以使用RL来表示。</p>
<h3 id="★-训练"><a href="#★-训练" class="headerlink" title="★ 训练"></a><strong>★ 训练</strong></h3><h4 id="☆-为相同的任务训练"><a href="#☆-为相同的任务训练" class="headerlink" title="☆ 为相同的任务训练"></a><strong>☆ 为相同的任务训练</strong></h4><p>RL并不需要任何的训练，所有的参数都是预先定义的，因而它与数据集是无关的。FV需要一个通过无监督学习（通过对数据集聚类局部特征）得到的视觉码表，此外它不需要依赖任何数据，而且与任务是独立的。为了解决分类问题，文档图像特征以及文档标签被用于训练分类器。在我们的所有实验中，均使用线性SVM分类器，连接在RL或FV特征的上面。</p>
<p>不同于RL与FV，CNN是一种深度学习方法，特征提取步骤和分类任务是用同一个架构来完成的。我们将选择AlexNet和GoogLeNet作为结构，使用交叉熵作为损失函数通过SGD（随机梯度下降法）进行优化，分别训练两个网络。此外，还使用了dropout以防止过拟合。</p>
<p>而混合表示的训练，就是以上两种情况的结合。</p>
<h4 id="☆-为不同的任务训练"><a href="#☆-为不同的任务训练" class="headerlink" title="☆ 为不同的任务训练"></a><strong>☆ 为不同的任务训练</strong></h4><p>RL与FV，描述符可以直接地被应用于其它的任务，唯一需要做的就是给它一个好的排序/预测/分类器。而对于CNN而言，除了直接端到端训练以完成既定任务以外，还可以单独地作为特征提取器来使用，深度网络往往可以提取更复杂的特征以及语义信息。这样一来，CNN也可以像RL与FV那样，后接其它的排序/预测/分类器，完成不同的任务。在CNN中，我们从不同深度的操作层中提取出特征进行实验，以对比效果。</p>
<p>本文中，我们通过一系列任务进行定量实验，包括：图像分类、图像检索、目标检测以及动作识别。它们都可以被泛化到文档图像中。</p>
<h3 id="★-评估"><a href="#★-评估" class="headerlink" title="★ 评估"></a><strong>★ 评估</strong></h3><h4 id="☆-数据集"><a href="#☆-数据集" class="headerlink" title="☆ 数据集"></a><strong>☆ 数据集</strong></h4><p><img src="http://static.zybuluo.com/jiangshuo1016/uf19o63vh0gjkeoc45ixerls/QQ%E6%88%AA%E5%9B%BE20181213170316.png" alt="QQ截图20181213170316.png-57.9kB"></p>
<p>本文一共使用了7个数据集，前4个是公开数据集，后3个是内部数据集。</p>
<p>※RVL-CDIP：共有400000幅带标签的图片，分为16类：信件、备忘录、邮件等等。<br>※NIST：共有5590幅图片，分为12类的税表。<br>※MARG：共有1553个文档，每个文档是医学杂志的封面，分为9类布局。<br>※CLEF-IP：38081个专利图像，分为9个类别：流程图、零件图、表格等等。</p>
<h4 id="☆-执行细节"><a href="#☆-执行细节" class="headerlink" title="☆ 执行细节"></a><strong>☆ 执行细节</strong></h4><p>RL：5层金字塔，q=11，特征是10648维。图片二值化后，重排列至250K像素。该特征是独立于数据集的，因此可以运用到所有的任务中。</p>
<p>FV：基于5种尺度的SIFT特征，原始的SIFT特征通过PCA降维至77维，再联合位置等信息添加到80维。考虑不同大小的视觉词表。对于一个新的数据集，可以考虑新的SIFT-PCA和GMM来建立FV，也可以使用旧的模型。</p>
<p>混合：RL+MLP，FV+MLP。不同尺寸的FV需要不同的混合模型。</p>
<p>CNN：两种结构的网络，初始化过程可以在线获得。使用ImageNet2012预训练的网络，其效果要好于通过数据集训练的网络。因此我们选择预训练好的网络，再对其根据数据集进行调整。</p>
<h3 id="★-实验"><a href="#★-实验" class="headerlink" title="★ 实验"></a><strong>★ 实验</strong></h3><p>实验分为两个部分，第一部分是使用RVL-CDIP数据集进行分类实验，第二部分是测试特征迁移到其它数据集的新任务上的表现。</p>
<h4 id="☆-第一部分"><a href="#☆-第一部分" class="headerlink" title="☆ 第一部分"></a><strong>☆ 第一部分</strong></h4><p><img src="http://static.zybuluo.com/jiangshuo1016/2hvs84xachi6nt6dediko9x4/image_1cul732tnp7va741e1ihetap9.png" alt="image_1cul732tnp7va741e1ihetap9.png-18.4kB"></p>
<p>在RVL-CDIP数据集上进行分类实验，每一种特征的实验都选择最佳的参数，结果准确率如表格中所示。首先可以看到，CNN的两种结构要好于另外的特征，其中CNN-G达到了SOTA。其次FV+MLP的混合特征，性能已经接近于CNN，而它训练速度快且不需要使用GPU。相比单纯的RL和FV，结合MLP可以明显地提升其性能。另外，本实验再次佐证了FV要好于RL。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/rpselsu5ko0jaj0in354b7g9/image_1cul8fs9r1e2i6tvm681nche9826.png" alt="image_1cul8fs9r1e2i6tvm681nche9826.png-13.5kB"></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/kr4ixm5xcnnnfmmktdpl5jq8/image_1cul8gu2b16go1pl113vv7rj78833.png" alt="image_1cul8gu2b16go1pl113vv7rj78833.png-25.7kB"></p>
<p>每个特征的实验中，还比较了参数选择对性能的影响，包括：FV的词表大小的选择，MLP隐藏层数以及PCA降维与否。实验发现，FV16与FV256要好于FV4；隐藏层数与PCA降维对FV的影响不大。</p>
<h4 id="☆-第二部分"><a href="#☆-第二部分" class="headerlink" title="☆ 第二部分"></a><strong>☆ 第二部分</strong></h4><p>这部分主要是探索如何将第一部分得到的特征应用于新的数据集以及新的任务。</p>
<p><strong>※深层特征</strong></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/0iv6mie7p64fb8pgyivbmxun/image_1cul8hmb81um1vvttabbl71lg63t.png" alt="image_1cul8hmb81um1vvttabbl71lg63t.png-80.8kB"></p>
<p>检索任务：测试了MAP，P@1，P@5。由于它们表现相似，所以表格里仅仅列出了MAP指标。CNN-G普遍要比CNN-A好；CNN-A-p5要好于CNN-A-fc6和fc7，可能是由于类别仅有16个。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/iaapd704xvdufi7vaxtxedds/image_1cul8lf0eiq31ih5l881s8ce7p6p.png" alt="image_1cul8lf0eiq31ih5l881s8ce7p6p.png-32.6kB"></p>
<p>聚类任务：以调整互信息（AMI）为指标，越大意味着聚类与真实情况越吻合。使用层次聚类法对它们进行聚类。CNN-G普遍要比CNN-A好。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/lrthiebolbjpbt8du4w87frw/image_1cul8mm9n572jd13rh1kep1rp176.png" alt="image_1cul8mm9n572jd13rh1kep1rp176.png-33.3kB"></p>
<p>分类任务：使用NCM分类器，它不依赖参数。本可以选用kNN分类器，但是k=1时，其结果就于检索相同，因此没有选择。CNN-G与CNN-A相似。</p>
<p><strong>※浅层特征</strong></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/lhnx2heg1qzhv4utnbe6vn5y/image_1cul8n58rkj9jt21o5lq0r18eq7j.png" alt="image_1cul8n58rkj9jt21o5lq0r18eq7j.png-71.3kB"></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/fyxvb850nq5cb39hfs236d24/QQ%E6%88%AA%E5%9B%BE20181214113653.png" alt="QQ截图20181214113653.png-61.2kB"></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/dl4zi7coh7mtnv0vf0u4d74g/image_1culcqdsta32rdi1mcnbbt1tciac.png" alt="image_1culcqdsta32rdi1mcnbbt1tciac.png-71.5kB"></p>
<p>在三项任务中，FV256+MLP在多数的数据集中总是表现最好，而PCA步骤会使性能稍稍降低。</p>
<p><strong>※对比</strong></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/woe3i4r8osbfr75o29pzj3xm/image_1culd9cvbf5l1aa51s663gj1038ap.png" alt="image_1culd9cvbf5l1aa51s663gj1038ap.png-144.7kB"></p>
<p><strong>针对CLEF-IP数据集进行分析：FV256+PCA要好于CNN，可能有以下两点原因：图片的尺寸各异，使得CNN表示不能很好地适应所有的图片；类内的差异大，从整体的布局不能很好地得到类别。</strong></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/4rjhepod2lylsd7d3hlrn38y/image_1culdiui5rao96kbsr1o5r1f0b6.png" alt="image_1culdiui5rao96kbsr1o5r1f0b6.png-150.2kB"><br><img src="http://static.zybuluo.com/jiangshuo1016/g2edu6rn2cddwrzrfyjl94xz/image_1culdok9q87a8q11t7rdfq1k6hc0.png" alt="image_1culdok9q87a8q11t7rdfq1k6hc0.png-69.5kB"></p>
<p>这张图是随机选取图片作为相似检索，紫色框表示输入，上方（左）是FV256+PCA给出的结果，下方（右）是CNN-G-i4e给出的结果。绿色框表示相关，红色框表示不相关。</p>
<h3 id="★-总结"><a href="#★-总结" class="headerlink" title="★ 总结"></a><strong>★ 总结</strong></h3><p>这篇文章为对比三种文档图像表示方法提出了详细的基准，三种图像表示方法分别是：浅层特征（RL/FV)，深层特征（CNN-A,CNN-G），混合特征。实验首先对比了这些特征在同一个数据集上进行分类任务的表现，然后再在不同的数据集和任务上进行实验，以测试它们的泛化能力。</p>
<p>实验结果显示，在不涉及领域迁移的情况下，CNN特征的表现最好，紧随其后的是混合特征，而且混合特征能够节省很多的训练损耗。这在自然图片中已经得到了证明，我们将结论推向了文档图像。在领域迁移中，混合特征的表现远不如浅层特征和深层特征。对于CNN而言，它可以适应于训练时任务差不多的目标任务，且在布局信息重要的数据集表现较好。而FV-PCA在图片尺寸差异较大、类内差异较大的数据集中表现得更好。</p>
<p><strong>自己的收获：</strong></p>
<ol>
<li>深入学习了Fisher Vector的数学基础。</li>
<li>对文档图像的特征表示，有了全局的认识。</li>
<li>CLEF-IP数据集的表示，更适合选用FV-PCA特征。</li>
<li>CLEF-IP数据集的检索实验，可以通过紫框绿框红框这种方式，可视化呈现结果。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="jrunning.cn/2018/12/11/paper-Detecting-figures-and/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiang Shuo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="保持动力">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/11/paper-Detecting-figures-and/" itemprop="url">论文笔记 | 专利图像检测：基于竞赛</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-11T21:53:00+08:00">
                2018-12-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper/" itemprop="url" rel="index">
                    <span itemprop="name">paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="专利中的图片检测以及零件标签检测"><a href="#专利中的图片检测以及零件标签检测" class="headerlink" title="专利中的图片检测以及零件标签检测"></a><strong>专利中的图片检测以及零件标签检测</strong></h2><blockquote>
<p>Riedl C, Zanibbi R, Hearst M A, et al. <strong>Detecting figures and part labels in patents: competition-based development of graphics recognition algorithms</strong>[J]. International Journal on Document Analysis and Recognition (IJDAR), 2016, 19(2): 155-172.</p>
</blockquote>
<h3 id="★-主要内容"><a href="#★-主要内容" class="headerlink" title="★ 主要内容"></a><strong>★ 主要内容</strong></h3><p>在USPTO的大多数专利文件里，包括了图片页来直接描述发明。这些页面里有图片以及一些数字标记，但是没有文本。所以读者必须要来回扫读文档来找到相关标记的描述。这个过程费时费力，通过自动创建的’tool-tips’与超链接可以改善这一问题。为此，USPTO举办了一场比赛，来检测图片以及部分零件的标记。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/8s9j79u5lrzzsset5klzwwqi/image_1cug34jqcs981m741rs64dda1s9.png" alt="image_1cug34jqcs981m741rs64dda1s9.png-322.8kB"></p>
<p>一幅图片中，可能包含单张图片，也可能包含多张图片。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/un3n4hgr2hkd5gl8gnkis0d5/Image.png" alt="Image.png-372.5kB"></p>
<p>图片中的标签可能使用不同的字体甚至手写，整幅图片也可能会旋转。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/f8y0ek9le1l8mx4eephidybt/image_1cug3jrig10pbj7817u61o1r1rdr7b.png" alt="image_1cug3jrig10pbj7817u61o1r1rdr7b.png-197.4kB"></p>
<p>图名的字体形式是各种各样的，图里的文字也是各种各样的。</p>
<h3 id="★-竞赛概况"><a href="#★-竞赛概况" class="headerlink" title="★ 竞赛概况"></a><strong>★ 竞赛概况</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th>系统输入</th>
<th>系统输出</th>
</tr>
</thead>
<tbody>
<tr>
<td>8位灰度图片（300dpi）</td>
<td>图片边界方框以及标题文本</td>
</tr>
<tr>
<td>附属的HTML专利文本</td>
<td>标签边界方框以及文本</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><a href="http://archive.ics.uci.edu/ml/datasets/USPTO+Algorithm+Challenge,+run+by+NASA-Harvard+Tournament+Lab+and+TopCoder++++Problem:+Pat" target="_blank" rel="noopener">竞赛数据集以及前五名源码</a></li>
<li><a href="https://community.topcoder.com/longcontest/stats/?&amp;sr=1&amp;nr=50&amp;module=ViewOverview&amp;rd=15027" target="_blank" rel="noopener">竞赛结果榜单</a></li>
</ul>
<p><strong>获胜队伍的解决方案</strong>是使用C++编程，唯一一只使用了预先提供的HTML文本来验证检测出的图名和零件标签。使用OCR（Optical Character Recognition，光学字符识别）的结果来检测页码数字和页眉，来帮助定位HTML文本中可能的标题和标签。</p>
<p><strong>自己的收获：</strong></p>
<ol>
<li>大致了解从专利中分割图片的工作以及专利中检测标签的工作。</li>
<li>知道了关于文档图像以及文档分析，有非常多的竞赛围绕开展。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="jrunning.cn/2018/12/06/paper-CLEF-IP-2011/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiang Shuo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="保持动力">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/06/paper-CLEF-IP-2011/" itemprop="url">论文笔记 | CLEF-IP 2011竞赛</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-06T21:53:00+08:00">
                2018-12-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper/" itemprop="url" rel="index">
                    <span itemprop="name">paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="CLEF-IP-2011竞赛：知识产权领域的检索"><a href="#CLEF-IP-2011竞赛：知识产权领域的检索" class="headerlink" title="CLEF-IP 2011竞赛：知识产权领域的检索"></a><strong>CLEF-IP 2011竞赛：知识产权领域的检索</strong></h2><blockquote>
<p>Piroi F, Lupu M, Hanbury A, et al. <strong>CLEF-IP 2011: Retrieval in the Intellectual Property Domain</strong>[C]//CLEF (notebook papers/labs/workshop). 2011.</p>
<p>竞赛主页：<a href="http://www.ifs.tuwien.ac.at/~clef-ip/download/2011/index.shtml" target="_blank" rel="noopener">http://www.ifs.tuwien.ac.at/~clef-ip/download/2011/index.shtml</a></p>
</blockquote>
<h3 id="★-引言"><a href="#★-引言" class="headerlink" title="★ 引言"></a><strong>★ 引言</strong></h3><p><a href="http://www.ifs.tuwien.ac.at/~clef-ip/download-central.shtml" target="_blank" rel="noopener">CLEF-IP</a>比赛与<a href="https://trec.nist.gov/tracks.html" target="_blank" rel="noopener">TREC</a>比赛，都是由作者所在机构举办的。（<a href="http://blog.sciencenet.cn/blog-1497049-1086605.html" target="_blank" rel="noopener">马舒天：2017TREC文本检索会议参会感想</a>）CLEF比赛是从2009年开始的，聚焦专利检索，为研究者提供了大量的干净数据，数据集包括欧洲专利局的专利，语言涵盖英语、德语、法语。2009年的竞赛中，主要是针对Prior Art；2010年，数据集进一步扩大，并且增加了专利分类的任务，目标是将专利自动地分配到正确的IPC类别中；<strong>2011年，增加了关于专利图像的任务，利用专利图像进行已有技术检索与分类</strong>。</p>
<p><a href="https://en.wikipedia.org/wiki/Prior_art" target="_blank" rel="noopener"><strong>Prior Art</strong></a>，在中文中可以翻译为已有技术。在知识产权领域，一项专利在授权前，都要经过审查员的严格检索，确保没有先前的相似实例公开，这种检索叫Prior Art Search。维基百科中，Prior Art又称state of the art(SOTA)或background art，SOTA我们就比较熟悉了，经常用于表示研究中目前达到的最好性能。</p>
<p>专利中的图像包括人手绘制的、电脑绘制的，或是两者混合。可能包括了文本，通常是黑白的。当专利专家在浏览一系列检索结果时，他可以根据图像快速地过滤掉许多不相关的专利，需要精读的专利数量就会得到大幅度的减少。</p>
<h3 id="★-相关数据收集"><a href="#★-相关数据收集" class="headerlink" title="★ 相关数据收集"></a><strong>★ 相关数据收集</strong></h3><p>专利的文档都是以XML文件的格式存储的，这个文档可能是：Application Document（申请文档），Search Report（搜索报告）或Granted Patent Document（授权文档）。分别用A1，A3，B1前缀加以区分。CLEF的专利文档XML数据，都是从<a href="http://www.ifs.tuwien.ac.at/imp/marec.shtml" target="_blank" rel="noopener">MAREC数据库</a>中摘取的，该数据库包含有19,386,697个专利文档，都是以XML格式存储的，总大小达到了621GB（现在好像无法下载了）。XML文件通常包含的文本信息有：著录资料，摘要，描述，权利要求。</p>
<p>专利文档方面，共有350万左右XML文件。图像方面，针对其中三个IPC子类的所有47000个专利（截止到2002年），截取图像共291,566个tiff格式的文件，共5.4GB。下面两张表是选择的三个IPC子类。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/vepnl85ayt08lfze7lvbzv0h/image_1cu1nbce0h091t7b1m0u16lj19iu68.png" alt="image_1cu1nbce0h091t7b1m0u16lj19iu68.png-18.8kB"></p>
<h3 id="★-竞赛概述"><a href="#★-竞赛概述" class="headerlink" title="★ 竞赛概述"></a><strong>★ 竞赛概述</strong></h3><ul>
<li>Prior Art Candidates Search(PAC)，已有技术专利检索</li>
<li>Patent Classication(CLS1)，专利分类到subclass</li>
<li>Refined Patent Classication(CLS2)，专利分类到group/subgroup</li>
<li>Patent Image-based Prior Art Search(IMG-PAC)，使用图片检索已有技术专利 </li>
<li>Patent Image-based Classication(IMG-CLS)，专利图片分类</li>
</ul>
<h4 id="☆-IMG-CLS"><a href="#☆-IMG-CLS" class="headerlink" title="☆ IMG-CLS"></a><strong>☆ IMG-CLS</strong></h4><p>目标自动地对专利的图像进行分类。在任务中，图像被单独截取出来，而不是整个页面直接拿来用。分类共包括9个类别：abstract drawing, graph, flowchart, gene sequence, program listing, symbol, chemical structure, table and mathematics。每个类别包括了300-6000张数量不等的图片，用于训练，训练不得用额外的数据。得到训练好的分类器后，再用1000张随机图片进行测试。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/1i9dsppidhiaeszuuoieyg74/image_1cu1ne07p1f121487rm4m0ubqt6l.png" alt="image_1cu1ne07p1f121487rm4m0ubqt6l.png-26kB"></p>
<h3 id="★-相关性评估"><a href="#★-相关性评估" class="headerlink" title="★ 相关性评估"></a><strong>★ 相关性评估</strong></h3><p>PAC与IMG-PAC是通过专利的引用来评估相关性的，这些被引用的专利都已经提前保存在数据集中。然而平均每个专利的引用数量低（低于4个），所以我们需要扩大一些相关文档的范围，把引用的引用也囊括其中。引用的提取过程，在2009年的竞赛中已经详细描述，可以参考文献：</p>
<blockquote>
<p>Roda G, Tait J, Piroi F, et al. <strong>CLEF-IP 2009: retrieval experiments in the Intellectual Property domain</strong>[C]//Workshop of the Cross-Language Evaluation Forum for European Languages. Springer, Berlin, Heidelberg, 2009: 385-409.</p>
</blockquote>
<p>对于CLS1和CLS2，由于类别是准确知道的，所以直接将分类编码提取就可以进行评估。而对于IMG-CLS，所有的评估都是手工标注完成的。</p>
<h3 id="★-参赛方案"><a href="#★-参赛方案" class="headerlink" title="★ 参赛方案"></a><strong>★ 参赛方案</strong></h3><p>一共有12支队伍参加了比赛，但是对于涉及专利图像的比赛，分别只有1支和4支队伍参加。以下提炼部分队伍的关键技术。</p>
<p>队伍1，参加PAC：使用Xtrieval框架（为许多检索引擎提供接口），在预处理中针对不同的专利类别创建停用词列表，每种语言的专利分别处理；对比了使用长句和短句的检索，发现长句效果更好。</p>
<p><strong>※队伍2</strong>，参加PAC：使用Lemur工具集，将其它语言都翻译成英语；应用了一种短语抽取技术来创建检索式，接着将得到的专利转换成基于IPC分类的文档向量来检查相关性。</p>
<p>队伍3，参加IMG-CLS：使用了图像的LBP，MPEG-7特征以及通过OCR得到的文本，利用SVM进行分类。实验了单一特征与多种特征的融合，最理想的方案是只使用LBP特征。</p>
<p>队伍4，参加PAC：使用PatTextTiling技术（基于TextTiling算法），对专利进行总结。对专利文档进行停用词过滤和词干还原后，使用Terrier进行编码。</p>
<p><strong>※队伍5</strong>，参加CLS：使用Linguistic Classification System来配置三种分类器：Navie Bayes，Winnow和SVM。在训练阶段，只使用的英语专利的摘要和描述（前400词），通过<a href="http://www.phasar.cs.ru.nl/aboutTM4IP.pdf" target="_blank" rel="noopener">AEGIR依存分析器</a>将本文转化为三元组。</p>
<p>队伍6，参加PAC：使用bag-of-words（词袋）的方法，来挖掘句法及语义信息。只使用了英语专利的标题、摘要、权利要求以及描述。检索是使用Spinque框架完成的，它提供用户一个可视化接口来完成检索策略的定义。</p>
<p>队伍7，参加PAC和CLS：从文档中挑选出赋予不同权重的关键词。然后进行词性标注和各种垃圾词的过滤，考虑剩下文本的词语共现情况。其它语言都通过MyMemory翻译服务转换成英语。整个任务是基于Lucene的，它是一个基于Java的接口。分类任务用的是KNN算法。</p>
<p><strong>※队伍8</strong>，参加了IMG-PAC和IMG-CLS：使用Fisher Vector来表示图像，通过线性分类器完成分类。为了扩大数据集，对原有训练集内的图像进行了旋转。对于专利文本的检索，不同部分的信息被赋予不同的权重。相似度是通过IPC类别以及引用图谱来融合计算的。由于利用图像信息检索的性能较差，所以将其融合至只利用文本信息的检索中，融合后的性能好于只使用文本信息检索。</p>
<h3 id="★-评价指标"><a href="#★-评价指标" class="headerlink" title="★ 评价指标"></a><strong>★ 评价指标</strong></h3><p>对于PAC任务，评价指标包括：</p>
<ul>
<li>Precision@1, Precision@5, Precision@10, Precision@20, Precision@50, Precision@100</li>
<li>Recall@1, Recall@5, Recall@10, Recall@20, Recall@50, Recall@100</li>
<li>MAP</li>
<li>NDCG</li>
</ul>
<p>对于CLS任务，评价指标包括：</p>
<ul>
<li>Precision@1, Precision@5</li>
<li>Recall@1, Recall@5</li>
<li>F1 at 1 and 5.</li>
</ul>
<p><strong>Precision：</strong>准确率，是返回的结果中相关文档所占的比例，@5表示检索5个文档。<br><strong>Recall：</strong>召回率，是返回的相关文档占所有相关文档的比例。<br><strong>F1：</strong>兼顾准确率和召回率的综合指标。<br><strong>MAP(Mean Average Precision)：</strong>单个主题的平均准确率，是每篇相关文档检索出后的准确率的平均值。<br><strong>NDCG(Normalized Discounted Cumulative Gain)：</strong>归一化折损累计增益，用于度量排序质量。</p>
<blockquote>
<p>参考资料：<a href="https://blog.csdn.net/u010138758/article/details/69936041/" target="_blank" rel="noopener">CSDN | 信息检索中常用的评价指标</a></p>
</blockquote>
<p>对于IMG-CLS任务，先画出<a href="https://www.plob.org/article/12476.html" target="_blank" rel="noopener">ROC曲线</a>，然后计算<a href="https://blog.csdn.net/zjm750617105/article/details/52558779" target="_blank" rel="noopener">EER（Equal Error Rate，等错误率）</a>和<a href="https://www.plob.org/article/12476.html" target="_blank" rel="noopener">AUC（Area Under Curve，曲线下面积）</a>。再计算每个类别的<a href="https://www.cnblogs.com/maybe2030/p/5375175.html" target="_blank" rel="noopener">TPR（True Positive Rate，真阳率）</a>及<a href="https://www.jianshu.com/p/27228ad417d4" target="_blank" rel="noopener">混淆矩阵</a>。</p>
<h3 id="★-最终结果"><a href="#★-最终结果" class="headerlink" title="★ 最终结果"></a><strong>★ 最终结果</strong></h3><p>针对五个比赛，对比了不同队伍在各个指标上的表现，横坐标均为队伍每一轮提交测试得到的性能。</p>
<h4 id="☆-PAC"><a href="#☆-PAC" class="headerlink" title="☆ PAC"></a><strong>☆ PAC</strong></h4><p><img src="http://static.zybuluo.com/jiangshuo1016/fk9yn70q75am5fbpy8mwruvp/image_1cu1ju99vobpsa91fhe1car14epp.png" alt="image_1cu1ju99vobpsa91fhe1car14epp.png-44.4kB"></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/9dmvs7hyc2p2avlr54wvnyn0/image_1cu1jvq2v82lnsqe8hhcrmic16.png" alt="image_1cu1jvq2v82lnsqe8hhcrmic16.png-50.2kB"></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/exo98l5volbwtuwq795e63bb/image_1cu1kcdcm1d43phf1suq1hhlcia5e.png" alt="image_1cu1kcdcm1d43phf1suq1hhlcia5e.png-56.7kB"></p>
<p><strong>结论：</strong>准确率普遍都不太高，@1大约0.5%，@100大约2%。召回率相对准确率而言较高，@1大约40%，@100大约25%。一次检索较多的数量能增加准确率，降低召回率。语言对于准确性有一定的影响，英语准确率较高。<strong>上文提到的队伍2和队伍5表现优秀。</strong></p>
<h4 id="☆-CLS1"><a href="#☆-CLS1" class="headerlink" title="☆ CLS1"></a><strong>☆ CLS1</strong></h4><p><img src="http://static.zybuluo.com/jiangshuo1016/ox81vvoi5sfegs2jj8qqr2wx/image_1cu1k18vd1l15ige9mfmqh1cv82d.png" alt="image_1cu1k18vd1l15ige9mfmqh1cv82d.png-36.9kB"></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/dob11lizxbuv8csl4e5lwy9d/image_1cu1kh4b71f7e18lt5cu1ala1lnt5r.png" alt="image_1cu1kh4b71f7e18lt5cu1ala1lnt5r.png-30.5kB"></p>
<h4 id="☆-CLS2"><a href="#☆-CLS2" class="headerlink" title="☆ CLS2"></a><strong>☆ CLS2</strong></h4><p><img src="http://static.zybuluo.com/jiangshuo1016/pjas5r8wgvcz6sj2mbv5ujou/image_1cu1k1lld7o6h1cmrhgqf17oq2q.png" alt="image_1cu1k1lld7o6h1cmrhgqf17oq2q.png-33.1kB"></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/ndr68o1yn3hov4n4k5anllt1/image_1cu1k60iq1a9ss5ihob19tqhv53k.png" alt="image_1cu1k60iq1a9ss5ihob19tqhv53k.png-30.5kB"></p>
<h4 id="☆-IMG-PAC"><a href="#☆-IMG-PAC" class="headerlink" title="☆ IMG-PAC"></a><strong>☆ IMG-PAC</strong></h4><p><img src="http://static.zybuluo.com/jiangshuo1016/6ogtiuo25qi876w596tqrsrl/image_1cu1k0o45nhld1teh03kd1nkj20.png" alt="image_1cu1k0o45nhld1teh03kd1nkj20.png-30.1kB"></p>
<p><strong>结论：</strong>召回率很高，达到95%，准确率在10%左右。</p>
<h4 id="☆-IMG-CLS-1"><a href="#☆-IMG-CLS-1" class="headerlink" title="☆ IMG-CLS"></a><strong>☆ IMG-CLS</strong></h4><p><img src="http://static.zybuluo.com/jiangshuo1016/h3gtlntyu670c9c6pgthn6ga/image_1cu1o4sgdgqr1lq7grn1gir1fgm72.png" alt="image_1cu1o4sgdgqr1lq7grn1gir1fgm72.png-51.4kB"></p>
<p><strong>结论：队伍8表现更优秀，方法值得借鉴。</strong></p>
<p><strong>自己的收获：</strong></p>
<ol>
<li>对于整个CLEF-IP竞赛有了完整的认识。</li>
<li>学习了信息检索的相关指标。</li>
<li>后续还将继续探索专利图像的运用，队伍8的方法值得学习。</li>
<li>由于IMG-CLS数据集量大且质量高，研究这个细分方向的学者应该都会引用这份工作，后续可以在引用这份工作的论文中继续寻找优秀读物。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="jrunning.cn/2018/12/04/paper-document-image-classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiang Shuo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="保持动力">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/04/paper-document-image-classification/" itemprop="url">论文笔记 | 文档图像分类：聚焦专利</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-04T21:53:00+08:00">
                2018-12-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper/" itemprop="url" rel="index">
                    <span itemprop="name">paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="文档图像分类，聚焦专利图像应用"><a href="#文档图像分类，聚焦专利图像应用" class="headerlink" title="文档图像分类，聚焦专利图像应用"></a><strong>文档图像分类，聚焦专利图像应用</strong></h2><blockquote>
<p>Csurka G. <strong>Document image classification, with a specific view on applications of patent images</strong>[M]//Current Challenges in Patent Information Retrieval. Springer, Berlin, Heidelberg, 2017: 325-350.</p>
</blockquote>
<h3 id="★-摘要"><a href="#★-摘要" class="headerlink" title="★ 摘要"></a><strong>★ 摘要</strong></h3><p>针对文档图像的分类与检索，分析比较了Run-Length Histogram与Fisher vector-based两种图像表示方法其它们不同参数下的表现。本文工作目标：指导面对不同任务时，如何选择相关参数。针对CLEF-IP 2011，预测图像类型，检索相关专利。本文使用的数据集包括：</p>
<ul>
<li>MARG benchmarks</li>
<li>two data sets built on customer data(1H1/NIT)</li>
<li><a href="http://www.ifs.tuwien.ac.at/~clef-ip/download/2011/index.shtml" target="_blank" rel="noopener">CLEF-IP 2011</a>（<em>专利图像数据集，留个坑，后续详细解读</em>）</li>
</ul>
<h3 id="★-引言"><a href="#★-引言" class="headerlink" title="★ 引言"></a><strong>★ 引言</strong></h3><p>专利图像很重要，但是不是每一张图片都一样重要。包含化学结构式、基因序列、电路等的图片，是最重要的。如果一次相似检索中，检出来的都是流程图或者表格，那么并不会给使用者带来多少帮助。如果只有一种类型的图像（drawing）被检索，那么相比于检索所有类型的图像，检索的准确率就有极高的提升。然而，这需要提前就确定图像的类别。人工标注图像的类别的工作，既不存在前人的工作，自己搞又容易出错。因此，我们需要自动地预测图像的类别。除此以外，本文还探索了相似图像检索的相关工作。</p>
<h3 id="★-文档图像表示"><a href="#★-文档图像表示" class="headerlink" title="★ 文档图像表示"></a><strong>★ 文档图像表示</strong></h3><p>现有若干方法只包含了相对有限的信息，它们可能针对某个特定的数据集表现好，但是不能很好地应对所有的类别、数据集以及任务。相对最早的自然图像表示（比如颜色直方图），bag-of-visual words (BOV) 表现地比它要很好很多，Fisher vector是最成功的BOV图像表示的一种；而本文的另一种方法run-length histograms也有更一般的普适性。</p>
<h4 id="☆-游程直方图（Run-length-histograms）"><a href="#☆-游程直方图（Run-length-histograms）" class="headerlink" title="☆ 游程直方图（Run-length histograms）"></a><strong>☆ 游程直方图（Run-length histograms）</strong></h4><p>主要思想是去编码同一方向上具有相同值像素的序列，对于文档中的图像而言，由于通常是黑白的，所以只需要两个通道即可。对于灰度图片，我们首先对其二值化，然后仅考虑黑白。对于彩色图片，我们可以将亮度信息二值化。二值化有许多方法，最简单的比如直接设定0.5的阈值，还有许多复杂的方法，还有探讨二值化的世界比赛：<a href="https://vc.ee.duth.gr/dibco2017/" target="_blank" rel="noopener">DIBCO</a>/<a href="http://vc.ee.duth.gr/h-dibco2018/" target="_blank" rel="noopener">HDIBCO</a>。</p>
<p>游程直方图可以将图像表示为 $(4×2×Q)$的特征矩阵，其中$(4)$表示4个方向：水平、竖直、对角线、反对角线，其中$(2)$表示黑与白两个通道，其中$(Q)$表示通道长度，一般取$Q=[\log_2(m)+2]$，$m$表示像素的数量，中括号表示取整。</p>
<p>举例，如果是一个$(3×3)$的矩阵（黑色$1$，白色$0$）：<script type="math/tex">\begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 1 & 1 & 0 \\ \end{bmatrix}</script></p>
<p>以水平为例，第一行尾巴连接到第二行的头形成一维序列，由一个黑游程，紧跟一个白游程，组成游程对，最后形成游程对序列。其中，黑色写在前面，白色写在后面。</p>
<ul>
<li>水平方向游程对序列：$<1,4><3,1>$</3,1></1,4></li>
<li>竖直方向游程对序列：$<1,1><1,2><1,1><1,1>$</1,1></1,1></1,2></1,1></li>
<li>对角方向游程对序列：$<1,2>$</1,2></li>
<li>反对方向游程对序列：$<0,2><1,0>$</1,0></0,2></li>
</ul>
<p>根据公式：$Q=[\log_2(9)+2]=5$</p>
<ul>
<li>水平方向RL矩阵 <script type="math/tex">=\begin{bmatrix} 1 & 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 1 & 0  \\ \end{bmatrix}</script></li>
<li>竖直方向RL矩阵 <script type="math/tex">=\begin{bmatrix} 4 & 0 & 0 & 0 & 0 \\ 3 & 1 & 0 & 0 & 0  \\ \end{bmatrix}</script></li>
<li>对角方向RL矩阵 <script type="math/tex">=\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0  \\ \end{bmatrix}</script></li>
<li>反对方向RL矩阵 <script type="math/tex">=\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0  \\ \end{bmatrix}</script></li>
</ul>
<p>此外，为了更好地挖掘图像中的布局信息，还有一种金字塔的方法，如下图所示。将图像按照这样去分割，形成$(1×1)(2×2)(4×4)$三种分割方法，一共得到21个区域。可以合并21个区域的表示，作为最终的图像特征的表示。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/7sdder7ydutgw5yurx3wm63y/image_1ctsrbvgqse3nlj1ghmka11parp.png" alt="image_1ctsrbvgqse3nlj1ghmka11parp.png-128.3kB"></p>
<p>为了使矩阵与图像的尺寸无关，我们可以对其进行$L1$规范化。</p>
<blockquote>
<p><strong>理解参考资料：</strong><br>徐德智, 童学锋, 宣国荣, et al. <strong>基于直方图调整的二值图像无损数据隐藏</strong>[J]. 计算机应用, 2009, 29(6):1651-1653.</p>
</blockquote>
<h4 id="☆-Fisher-Vector（Fisher向量）"><a href="#☆-Fisher-Vector（Fisher向量）" class="headerlink" title="☆ Fisher Vector（Fisher向量）"></a><strong>☆ Fisher Vector（Fisher向量）</strong></h4><p>Fisher vector本质上是用似然函数的梯度vector来表达一幅图像，它从图像的一种特征向量SIFT中挖掘了出更丰富的信息。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/28ln233qcvv2am1wsnct5a1m/image_1ctss5d4g7c31cq7ul018gh1cbn1j.png" alt="image_1ctss5d4g7c31cq7ul018gh1cbn1j.png-128.9kB"></p>
<blockquote>
<p><strong>理解参考资料：</strong><br><a href="https://blog.csdn.net/happyer88/article/details/46576379" target="_blank" rel="noopener">资料一：Fisher Vector 学习笔记</a><br><a href="https://blog.csdn.net/breeze5428/article/details/32706507" target="_blank" rel="noopener">资料二：Fisher Vector coding</a><br><a href="https://blog.csdn.net/ikerpeng/article/details/41644197" target="_blank" rel="noopener">资料三：Fisher Vector 通俗学习</a></p>
</blockquote>
<h3 id="★-数据集"><a href="#★-数据集" class="headerlink" title="★ 数据集"></a><strong>★ 数据集</strong></h3><p><img src="http://static.zybuluo.com/jiangshuo1016/v7pjx1rp9r7bkin5g4dn8omn/image_1ctssafl82541ip21nb9lrd1ohd43.png" alt="image_1ctssafl82541ip21nb9lrd1ohd43.png-30.4kB"></p>
<p>重点关注CLEF-IP数据集，它是一个专利图像分类任务的数据集，它用于2011年的比赛。这个比赛将专利中的图片分类到预先定义好的类别中去，一共包括9个类别（abstract drawing, graph, flowchart, gene sequence, program listing, symbol, chemical structure, table and mathematics）。数据集中每一类共包含了300-6000不等数量的标记图像，一共38081张。图像的分辨率从1500像素到4.5M像素不等。</p>
<h3 id="★-实验设置与评估方法"><a href="#★-实验设置与评估方法" class="headerlink" title="★ 实验设置与评估方法"></a><strong>★ 实验设置与评估方法</strong></h3><p>主要目的是选出好的图像表示方法，以及为这个方法选择好的参数，而参数的选择可能是依赖于任务本身的。在检索任务中，使用了不同的分类器（SVM/KNN/NCM），观察不同参数下实验结果。相对SVM与KNN，NCM自身没什么参数要设置。</p>
<h3 id="★-实验结果"><a href="#★-实验结果" class="headerlink" title="★ 实验结果"></a><strong>★ 实验结果</strong></h3><h4 id="☆-RL"><a href="#☆-RL" class="headerlink" title="☆ RL"></a><strong>☆ RL</strong></h4><p><img src="http://static.zybuluo.com/jiangshuo1016/6h6b8k571fic4pwv1tjpjy0i/image_1ctsshmgl5mqrmr12j219o8ofg4g.png" alt="image_1ctsshmgl5mqrmr12j219o8ofg4g.png-334.2kB"></p>
<p>S表示不同的分辨率，L表示金字塔的层数（1×1/2×2/4×4），Q表示量子化间隔（RL矩阵里那个参数）。<br><strong>结论：</strong>对于量子化间隔，最好是11；金字塔层数，针对MARG和NIT最好是4或5，而针对其他数据集，就不定；针对图片尺寸，最佳参数也不定，但是S0多一些，即不使用金字塔。针对检索任务，KNN表现最好（k=1时）而MAP表现最差。针对分类任务，SVM在CLEF-IP上表现更好，这是由于这个数据集更大，更容易让SVM学习。</p>
<h4 id="☆-FV"><a href="#☆-FV" class="headerlink" title="☆ FV"></a><strong>☆ FV</strong></h4><p><img src="http://static.zybuluo.com/jiangshuo1016/2xbx1l59nu2oiud3kzsvq1i4/image_1ctssjttqotu1tpu14k21gv01m7r5t.png" alt="image_1ctssjttqotu1tpu14k21gv01m7r5t.png-351.9kB"></p>
<p>W表示SIFT描述符的尺寸（24/32/48/64）；F表示降维所至的维数（48/64/96）；G表示高斯混合模型中词的数量。<br><strong>结论：</strong>针对特征尺寸，降维搭到48维最合适；极端地选择G可能会导致最好或最坏的结果，可能折中是比较好的选择；针对SIFT的尺寸，W3通常取得最好效果。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/smtozak88vxloitc2zwk35ul/image_1ctssleu31ndnn651tt4183at0q6n.png" alt="image_1ctssleu31ndnn651tt4183at0q6n.png-343kB"></p>
<p>S表示图片尺寸（50K/100K/250K/500K）；M表示特征尺寸以根号2缩放了几次；G的含义不变。<br><strong>结论：</strong>针对图片尺寸，高分辨率更好一些，如S4；针对特征尺寸的缩放次数，不定。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/t3p2iyl07ry3hefadel0h7dz/image_1ctssmn601ua1p8i17ie1etvm0c7h.png" alt="image_1ctssmn601ua1p8i17ie1etvm0c7h.png-177.7kB"></p>
<p>针对FV使用金字塔方法继续试验，L表示金字塔的层数，可以看到不同的分类器对于不同L的选择表现不同。</p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/gf1u2k9bu3qiqiir2diti6gy/image_1ctsso90obrnlq115rbqfrlc29.png" alt="image_1ctsso90obrnlq115rbqfrlc29.png-181.6kB"></p>
<p>这里作者又将基于SIFT的FV与XRCE上模型做了比较，全面吊打，说明本文研究成果好于当时那个竞赛。</p>
<h4 id="☆-RL-FV"><a href="#☆-RL-FV" class="headerlink" title="☆ RL+FV"></a><strong>☆ RL+FV</strong></h4><p><img src="http://static.zybuluo.com/jiangshuo1016/ud9lutv3m5bg3r0lbzahex9m/image_1ctssqjobjo01tf518kedooq89m.png" alt="image_1ctssqjobjo01tf518kedooq89m.png-156kB"></p>
<p>最自然直接的方式结合RL和FV就是早期或后期的融合。早期融合就是相加作点积，后期融合就是点积后求和。两者是等价的。作者做了一组固定参数的实验，从结果可以看到，FV普遍好于RL，而融合后的往往更好。</p>
<h3 id="★-基于图像的专利检索"><a href="#★-基于图像的专利检索" class="headerlink" title="★ 基于图像的专利检索"></a><strong>★ 基于图像的专利检索</strong></h3><p><strong>图像都使用刚才实验出来最好的参数（S3/W2/F2/G5/L1）表示成Fisher Vector（刚才所做的工作都是为了这一刻），作点积表示相似度。因为一个专利里面有多幅图片，所以专利与专利之间会形成多对图片见的相似度。一种考虑是所有相似度取平均，另一种考虑是只取所有里的最大值。在实验中，把所有类别不是drawing的图片全部去掉，在其他类别中，化学结构式图和基因序列图更为有用。</strong></p>
<p><img src="http://static.zybuluo.com/jiangshuo1016/au6zi959ymd1cr8rx81me2k6/image_1ctst31oi1urf1npo1hd91a3k15l623.png" alt="image_1ctst31oi1urf1npo1hd91a3k15l623.png-31.8kB"></p>
<p>实验的结果显示，所有的方法，性能都很差。这是由于专利不一定包含它内容的图片，但是我们可以用它融合到文本检索中去提升文本检索的性能，我们可以通过更复杂的方式来融合文本和图像。</p>
<blockquote>
<p>Ah-Pine J, Csurka G, Clinchant S. <strong>Unsupervised visual and textual information fusion in CBMIR using graph-based methods</strong>[J]. ACM Transactions on Information Systems (TOIS), 2015, 33(2): 9.</p>
</blockquote>
<p><strong>可以进一步研究的地方：</strong>（1）图像的表示可以用深度卷积神经网络。</p>
<blockquote>
<p>Harley A W, Ufkes A, Derpanis K G. <strong>Evaluation of deep convolutional nets for document image classification and retrieval</strong>[C]//Document Analysis and Recognition (ICDAR), 2015 13th International Conference on. IEEE, 2015: 991-995.</p>
<p>Kang L, Kumar J, Ye P, et al. <strong>Convolutional neural networks for document image classification</strong>[C]//Pattern Recognition (ICPR), 2014 22nd International Conference on. IEEE, 2014: 3168-3172.</p>
</blockquote>
<p>（2）可以把所有的图片连在一起看作是一个多页的文档，然后用相关的方法去分类和检索多页分档。</p>
<blockquote>
<p>Gordo A, Perronnin F. <strong>A bag-of-pages approach to unordered multi-page document classification</strong>[C]//2010 International Conference on Pattern Recognition. IEEE, 2010: 1920-1923.</p>
<p>Rusiñol M, Frinken V, Karatzas D, et al. <strong>Multimodal page classification in administrative document image streams</strong>[J]. International Journal on Document Analysis and Recognition (IJDAR), 2014, 17(4): 331-341.</p>
</blockquote>
<p>（3）可以在图像类别分类任务时结合文本信息。文本信息可以来源于正文中，也可以是通过OCR识别图中的文本再融合。</p>
<h3 id="★-总结"><a href="#★-总结" class="headerlink" title="★ 总结"></a><strong>★ 总结</strong></h3><p>本文主要在图像表示方面，介绍了RL和FV方法，然后针对检索任务和分类任务进行了大量的调参实验，获得了相对较好的参数。对RL与FV以及RL-FV作了对比，融合的表示模型性能更好。最后讨论了在CBIR方面的应用，单纯用图片来进行检索效果不好，但可以用它融合到文本检索中去提升文本检索的性能。</p>
<p><strong>自己的收获：</strong></p>
<ol>
<li>学习到了RL与FV的图像表示方法（FV没有完全搞懂）。</li>
<li>在参数的选择上，学习到了本文作者的实验方式。</li>
<li>得以深入挖掘CLEF-IP数据库，为专利图像分类研究打下基础。</li>
<li>本文CBIR部分，提到的前两个点比较有意思，可以延伸阅读。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.png"
                alt="Jiang Shuo" />
            
              <p class="site-author-name" itemprop="name">Jiang Shuo</p>
              <p class="site-description motion-element" itemprop="description">SJTU | PhD Candidate</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="mailto:jsmech@sjtu.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://instagram.com/shuo0571" target="_blank" title="Instagram">
                      
                        <i class="fa fa-fw fa-instagram"></i>Instagram</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.linkedin.com/in/%E6%9C%94-%E5%A7%9C-743155159/" target="_blank" title="Linkedin">
                      
                        <i class="fa fa-fw fa-linkedin"></i>Linkedin</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="/uploads/wechat-QR2.jpg" target="_blank" title="个人微信">
                      
                        <i class="fa fa-fw fa-wechat"></i>个人微信</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiang Shuo</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>





    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
	<span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
